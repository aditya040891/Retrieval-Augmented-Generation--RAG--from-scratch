{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Create and run a local RAG pipeline from scratch"
      ],
      "metadata": {
        "id": "SHyfJAKJmb4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is RAG?\n",
        "\n",
        "RAG stands for Retrieval Augmented Generation.\n",
        "\n",
        "The goal of RAG is to take information and pass it to an LLM so it can generate outputs based on that information.\n",
        "\n",
        "* Retrieval - Find relevant information given a query, e.g. \"what are the macronutrients and what do they do?\"  -> retrieve passages of text related to the macronutrients from a nutrition textbook.\n",
        "\n",
        "* Augmented - We want to take the relevant information and augment our input (prompt) to an LLM with that relevant information.\n",
        "\n",
        "* Generation - Take the first two steps and pass them to an LLM for generative outputs.\n",
        "\n",
        "If you want to read where RAG came from, see the paper from Facebook AI: https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf\n",
        "\n",
        "> This work offers several positive societal benefits over previous work: the fact that it is more\n",
        "strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\n",
        "with generations that are more factual, and offers more control and interpretability. RAG could be\n",
        "employed in a wide variety of scenarios with direct benefit to society, for example by endowing it\n",
        "with a medical index and asking it open-domain questions on that topic, or by helping people be more\n",
        "effective at their jobs."
      ],
      "metadata": {
        "id": "Vw70Kk_Nmqnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why RAG?\n",
        "\n",
        "The main goal of RAG is to improve the generation outputs of LLMs.\n",
        "\n",
        "1. Prevent hallucinations - LLMs are incredibly good at generating good *looking* text, however, this text does not mean that it is factual. RAG can help LLMs generate information based on relevant passages that are factual.\n",
        "\n",
        "2. Work with custom data - Many base LLMs are trained with internet-scale data. This means they have a fairly good understanding of language in general. However, it also does mean a lot of their responses can be generic in nature. RAG helps to create specific responses based on specific documents (e.g. your own companies customer support documents).\n"
      ],
      "metadata": {
        "id": "CvwtNmufnbtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What can RAG be used for?\n",
        "\n",
        "* Customer support Q&A chat - Treat your existing support documents as a resource and when a customer asks a question, you could have a retrieval system , retrieve relevant documentation snippets and then have an LLM craft those snippets into and answer. Think of this as a \"chatbot\" for your documentation.\n",
        "\n",
        "* Email chain analysis - Let's say you're a large insurance company and you have chains and chains of emails of customer claims. You could use a RAG pipeline to find relevant information from those emails and then use an LLM to process that information into structured data.\n",
        "\n",
        "* Company internal documentation chat\n",
        "\n",
        "* Textbook Q&A - Let's say you are a nutrition student and you have got a 1200 page textbook to read, you could build a RAG pipeline to go through the textbook and find relevant passages to the questions you have.\n",
        "\n",
        "Common theme here: take your relevant documents to a query and process them with an LLM.\n",
        "\n",
        "From this angle, you can consider an LLM as a calculator for words.\n"
      ],
      "metadata": {
        "id": "Wi5j5s4anckI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Local ?\n",
        "\n",
        "Fun.\n",
        "\n",
        "Privacy, speed and cost.\n",
        "\n",
        "* Privacy - If you have private documentation, may be you do not want to send that to an API. You want to setup an LLM and run it on your own hardware.\n",
        "\n",
        "* Speed - Whenever you use an API, you have to send some kind of data across the internet. This takes time. Running locally means we do not have to wait for transfers of data.\n",
        "\n",
        "* Cost - If you own your hardware, the cost is paid. It may have a large cost to begin with. But overtime, you do not have to keep paying API fees.\n",
        "\n",
        "* No vendor lockin - If you run your own software/hardware. If OpenAI/another large internet company shut down tomorrow, you can still run your business."
      ],
      "metadata": {
        "id": "PIBCJTKenc2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2GglnEXndMG",
        "outputId": "9b25c9b7-a5c7-48c2-8f8b-3c433e8ce869"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What we are going to build\n",
        "\n",
        "* https://github.com/mrdbourke/simple-local-rag\n",
        "* https://whimsical.com/simple-local-rag-workflow-39kToR3yNf7E8kY4sS2tjV\n",
        "\n",
        "We are going to build NutriChat to \"chat with a nutrition textbook\".\n",
        "\n",
        "Specifically:\n",
        "\n",
        "1. Open a PDF document (you could use almost any PDF here or even a collection of PDFs).\n",
        "\n",
        "2. Format the text of the PDF textbook ready for an embedding model.\n",
        "\n",
        "3. Embed all of the chunks of text in the textbook and turn them into numerical representations which we can store for later.\n",
        "\n",
        "4. Build a retrieval system that uses vector search to find relevant chunk of text based on a query.\n",
        "\n",
        "5. Create a prompt that incorporates the retrieved pieces of text.\n",
        "\n",
        "6. Generate an answer to a query based on the passages of the textbook with an LLM.\n",
        "\n",
        "1. Steps 1-3: Document preprocessing and embedding creation.\n",
        "\n",
        "2. Steps 4-6: Search and answer"
      ],
      "metadata": {
        "id": "EymyIovgndgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Document/text processing and embedding creation\n",
        "\n"
      ],
      "metadata": {
        "id": "bMovJ19znd0V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IFbzznljneX0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}